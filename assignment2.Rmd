---
title: "IDA Assignment 2"
author: "Johnny Lee, s1687781"
date: "25th March 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", "English")
library(maxLik)
load("dataex2.Rdata")
```

# Q1.

Suppose $Y_1, \cdots , Y_n$ are independent and identically distributed
with cumulative distribution function given by
$$F(y;\theta) = 1 - e^{-y^2/(2\theta)}, \quad y \geq 0, \quad \theta > 0.$$
Further suppose that observations are (right) censored if $Y_i > C$, for
some known $C > 0$, and let \begin{equation*}
  X_i=
  \begin{cases}
    Y_i \quad \text{if} \quad Y_i \leq C,\\
    C \quad \text{if} \quad Y_i > C,
  \end{cases}
  \quad
  R_i=
  \begin{cases}
    1 \quad \text{if} \quad Y_i \leq C\\
    0 \quad \text{if} \quad Y_i > C
  \end{cases}
\end{equation*}

## a)

Show that the maximum likelihood estimator based on the observed data
$\{(x_i,r_i)\}^{n}_{i=1}$ is given by
$$\hat{\theta} = \frac{\sum_{i=1}^n X_i^2}{2\sum_{i=1}^nR_i}.$$

### **Answer** :

We first define the Survival function (from **Workshop 3**)as
$$S(C;\theta)=\mathbb{P}(Y_i>C;\theta)=1-F(y_i;\theta)$$ which also
represents the censored observations. For the uncensored observations,
we have
$$f(y_i;\theta)=\frac{d}{dy_i}F(y_i;\theta)=\frac{ye^{-y^2/2\theta}}{\theta}$$
Given that $Y_1,\dots,Y_n$ are independent and identically distributed,
we have the likelihood function as,
\begin{equation}\label{eq:likelihood}
  \begin{split}
    L(\theta|\boldsymbol{y,r})&= \prod^{n}_{i=1} \bigg( [f(y_i;\theta)]^{r_i} [S(C;\theta)]^{1-r_i}\bigg)\\
    &= \prod^{n}_{i=1} \bigg( [\frac{ye^{-y^2/2\theta}}{\theta}]^{r_i} [e^{-C^2/\theta}]^{1-r_i}\bigg)\\
    &= \bigg( \frac{y_i}{2\theta}\bigg)^{\sum^{n}_{i=1}r_i}\exp\bigg(\frac{\sum^{n}_{i=1}(r_i y^2_i +(1-r_i)C^2)}{2\theta}\bigg)
  \end{split}
\end{equation}

Now we can rewrite the term in the exponential as $X_i$ can we expressed
as $$x_i=r_iy_i+C(1-r_i)$$ Then by taking square on both sides we have,
$$x_i^2=r_i^2y_i^2+(1-r_i)^2C^2+2r_iy_iC(1-r_i)$$ Noting that $R_i$ is
binary, we can then conclude with the expression as
\begin{equation} \label{eq:x}
  \begin{split}
    x_i^2=r_iy_i^2+(1-r_i)C^2
  \end{split}
\end{equation}

Now we substitute (\ref{eq:x}) into (\ref{eq:likelihood}) to have,
\begin{equation}\label{eq:loglik}
  \begin{split}
    L(\theta|\boldsymbol{y,r}) &= \bigg( \frac{y_i}{2\theta}\bigg)^{\sum^{n}_{i=1}r_i}\exp\bigg(\frac{\sum^{n}_{i=1}x_i}{2\theta}\bigg)\\
    \implies \log(L(\theta|\boldsymbol{y,r}))&=-\log\theta\sum^{n}_{i=1}r_i-\frac{\sum^{n}_{i=1}x_i^2}{2\theta}\\
    \implies \frac{d}{d\theta}\log L(\theta|\boldsymbol{y,r})&=\frac{1}{\theta}\sum^{n}_{i=1}r_i + \frac{1}{2\theta^2}\sum^{n}_{i=1}x_i^2
    \end{split}
\end{equation} By equating the derivative to $0$, we can obtain the
maximum likelihood estimate of $\theta$ as below.
$$\hat{\theta}_{MLE} = \frac{\sum_{i=1}^n x_i^2}{2\sum_{i=1}^nr_i} \quad \text{(shown)}$$.

\newpage

## b)

Show that the expected Fisher Information for the observed data
likelihood is $$I(\theta) = \frac{n}{\theta^2}(1 - e^{-C^2/(2\theta)})$$

**Note:**
$\int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/(2\theta)} + 2\theta(1 - e^{-C^2 / (2\theta)})$,
where $f(y;\theta)$ is the density function corresponding to the
cumulative distribution function $F(y;\theta)$ defined above.

### **Answer** :

From (\ref{eq:loglik}), we take another derivative of it and thus obtain
as below
$$\frac{d^2}{d\theta^2}\log L(\theta)=\frac{1}{\theta^2}\sum^{n}_{i=1}r_i-\frac{x_i^2}{\theta^3}$$
Then, the Fisher Information for the observed data likelihood is,
\begin{equation}\label{eq:fi}
  \begin{split}
    I(\theta|\boldsymbol{x,r})&=-\mathbb{E}\bigg(\frac{\sum^{n}_{i=1}r_i}{\theta^2}-\frac{x^2_i}{\theta^3}\bigg)\\
    &=-\frac{n\mathbb{E}(R)}{\theta^2}+\frac{n\mathbb{E}(X^2)}{\theta^3}\\
    I(\theta|\boldsymbol{y,r})&=-\frac{n\mathbb{E}(R)}{\theta^2}+\frac{1}{\theta^3}\bigg(n\mathbb{E}(RY^2)+nC^2\mathbb{E}(1-R)\bigg)
  \end{split}
\end{equation}

Again, noting that $R_i$ is binary, \begin{equation}\label{eq:expR}
  \begin{split}
    \mathbb{E}(R)&=1\cdot\mathbb{P}(R=1)+0\cdot\mathbb{P}(R=0)\\
    &=\mathbb{P}(R=1)=\mathbb{P}(Y\leq C)\\
    &=F(C;\theta) = 1-e^{-C^2/2\theta}
  \end{split}
\end{equation}

With the given equation,
$\mathbb{E}(RY^2)=\int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/(2\theta)} + 2\theta(1 - e^{-C^2 / (2\theta)})$,
we can combine all the above equations as express the expected Fisher
Information again,

\begin{equation}
  \begin{split}
    I(\theta|\boldsymbol{y,r})&=\frac{n\mathbb{E}(R)}{\theta^2}+\frac{1}{\theta^3}\bigg(n\mathbb{E}(RY^2)+nC^2\mathbb{E}(1-R)\bigg)\\
    &=\frac{-n}{\theta^2}(1-e^{-C^2/2\theta})-\frac{n}{\theta^3}(C^2e^{-C^2/2\theta})+\frac{n}{\theta^3}(2\theta(1-e^{-C^2/2\theta}))+\frac{n}{\theta^3}(C^2e^{-C^2/2\theta}) \\
    &=\frac{n}{\theta^2}(1-e^{-C^2/2\theta}) \quad \text{(shown)}
  \end{split}
\end{equation}

\newpage

## c)

Appealing to the asymptotic normality of the maximum likelihood
estimator, provide a $95\%$ confidence interval for $\theta$.

### **Answer** :

By the Central Limit Theorem, asymptotic normality of the maximum likelihood estimator is given as, 
<!-- $$\sqrt{n}(\hat{\theta}_{MLE}-\theta)\sim N_p(0, I(\theta)^{-1})$$.  -->
$$\hat{\theta}_{MLE}\sim N_p(\theta, I(\theta)^{-1})$$
Thus, with $0$ and $\frac{1}{I(\theta)}$ as the asymptotic mean and variance
respectively, we can obtain the $95\%$ confidence interval as below,
  $$\hat{\theta}_{MLE}\pm \frac{1.96}{\sqrt{I(\theta)}}=\hat{\theta}_{MLE}\pm\frac{1.96\cdot\theta_{MLE}}{\sqrt{n(1-e^{-C^2/2\theta_{MLE}})}}$$
  <!-- $$\hat{\theta}_{MLE}\pm \frac{1.96}{\sqrt{nI(\theta)}}=\hat{\theta}_{MLE}\pm\frac{1.96\cdot\theta_{MLE}}{\sqrt{n^2(1-e^{-C^2/2\theta_{MLE}})}}$$ -->

\newpage

# Q2.

Suppose that a dataset consists of $100$ subjects and $10$ variables.
Each variable contains $10\%$ of missing values. What is the largest
possible subsample under a complete case analysis? What is the smallest?
Justify.

Suppose that $Y_i \overset{\text{iid}}{\sim} N(\mu, \sigma^2)$ are iid
for $i=1,...,n$. Further suppose that now observations are (left)
censored if $Y_i < D$, for some known $D$ and let

\begin{equation*}
  X_i=
  \begin{cases}
    Y_i \quad \text{if} \; Y_i \geq D,\\
    D \quad \text{if} \; Y_i < D, 
  \end{cases}
  \quad
  R_i=
  \begin{cases}
    1 \quad \text{if} \; Y_i \geq D\\
    0 \quad \text{if} \; Y_i < D
  \end{cases}
\end{equation*}

## a)

Show that the log-likelihood of the observed data
$\{(x_i,r_i)\}^{n}_{i=1}$ is given by

$$\log L(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r}) = \sum_{i=1}^n\left\{r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu,     \sigma^2)\right\}$$

where $\phi(\cdot;\mu,\sigma^2)$ and $\Phi(\cdot;\mu, \sigma^2)$ stands,
respectively, for the density function and cumulative distribution
function of the normal distribution with mean $\mu$ and variance
$\sigma^2$.

### **Answer** :

We first define the Survival function (from **Workshop 3**)as 
$$S(D;\mu,\sigma^2)=\mathbb{P}(Y_i<D;\mu,\sigma^2)=\Phi(x_i;\mu,\sigma^2)$$
which also represents the censored observations. For the uncensored observation, we have
$$\phi(x_i;\mu,\sigma^2)$$

Given that $X_1,\dots,X_n$ are independent and identically distributed,
we have the likelihood function as,


\begin{equation}
  \begin{split}
    L(\mu,\sigma^2|\boldsymbol{x,r}) &= \prod^{n}_{i=1}\bigg([\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu,\sigma^2)]^{1-r_i}\bigg)\\
    \implies l(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r})&=\log \prod_{i=1}^{n}\bigg(\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu, \sigma^2)]^{1-r_i}\bigg)\\
    &= \log \bigg([\phi(x_i;\mu,\sigma^2)]^{\sum^{n}_{i=1} r_i}[\Phi(x_i;\mu, \sigma^2)]^{\sum^{n}_{i=1}(1-r_i)}\bigg)\\
    &=  \sum_{i=1}^n\bigg(r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu, \sigma^2)\bigg)
  \end{split}
\end{equation}

\newpage

## b)

Determine the maximum likelihood estimate of $\mu$ based on the data
available in the file `dataex2.Rdata`. Consider $\sigma^2$ known and
equal to $1.5^2$. **Note**: You can use a built in function such as
`optim` or the `maxLik` package in your implementation.

### **Answer** :

```{r}
#defining a function to simulate the log likelikhood
log.lik <- function(mu, data){
  x <- data[, 1]
  r <- data[, 2]
  sum((r*dnorm(x, mu, 1.5, log = TRUE) + 
         (1-r)*pnorm(x, mu, 1.5, log = TRUE)))
}

#computing the maximum likelihood estimate of mu
mle <- maxLik(logLik = log.lik, data = dataex2, start = c(mu = 5))
summary(mle)
```

We built a function `log.lik()` that produces the log likelihood and then used `maxLik()` to simulate $\mu$ based on the data. With Newton-Raphson method, we estimated $\hat{\mu}=5.5328$ and standard error of $0.1075$

\newpage

# Q3.

Consider a bivariate normal sample $(Y_1, Y_2)$ with parameters
$\theta=(\mu_1,\mu_2,\sigma_1^2,\sigma_{12},\sigma_2^2)$. The variable
$Y_1$ is fully observed, while some values of $Y_2$ are missing. Let $R$
be the missingness indicator, taking the value 1 for observed values and
0 for missing values. For the following missing data mechanisms state,
justifying, whether they are ignorable for likelihood-based estimation.

## a)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=\psi_0+\psi_1y_1, \quad \psi= (\psi_0,\psi_1)$
distinct from $\theta$.

### **Answer** :

Referring to the ignorability assumption (from **Lecture 6.1**), the missing in $Y_2$ is either **MAR** or **MCAR** and its model parameters, $\theta = (\mu_1, \mu_2, \sigma_1^2, \sigma_{12}, \sigma_2^2)$ and missing mechanism parameter, $\psi$.

First, the missing mechanism is **MAR**. This is because the missingness is only dependent on $Y_1$ which is a fully observed variable. The parameters, $\{\theta,\psi\}$ are also distinct. Therefore, the ignorability assumption holds here and (a) is ignorable for likelihood-based estimation.

## b)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=\psi_0+\psi_1y_2, \quad \psi= (\psi_0,\psi_1)$
distinct from $\theta$.

### **Answer** :

The missing mechanism is **MNAR** as the mechanism is only dependent on $Y_2$. Therefore, the missing value is depending on itself and possibly other factors. Therefore, by referring to the ignorability assumption (from **Lecture 6.1**), we conclude that (b) is not ignorable for likelihood-based estimation.

### c)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=0.5(\mu_1+\psi y_1)$,
scalar $\psi$ distinct from $\theta$.

### **Answer** :

The missing mechanism here is dependent on both $\mu_1$ and $Y_1$. We can observe similarity to (a). Distinctness of the parameters means that the parameter space of $(\theta, \psi)$ is equal to the Cartesian product of their individual product spaces. However, the $\mu_1$ also exists in the parameter space. This violates the ignorability assumption. Hence, (c) is not ignorable for likelihood-based estimation.

\newpage

# Q4.

$$Y_i\overset{\text{ind.}}{\sim} \text{Bernoulli}(p_i(\boldsymbol{\beta}))$$

$$p_i(\boldsymbol{\beta}) = \frac{exp(\beta_0 + x_i\beta_1)}{1 + exp(\beta_0 + x_i\beta_1)},$$
for $i=1,\cdots,n$ and $\boldsymbol{\beta} = (\beta_0, \beta_1)'$.
Although the covariate $x$ is fully observed, the response variable $Y$
has missing values. Assuming ignorability, derive and implement the EM
algorithm to compute the MLE of $\boldsymbol{\beta}$ based on the data
available in `dataex4.Rdata`. **Note**: $1$) For simplicity, and without
loss of generality because we have a univariate pattern of missingness,
when writing down your expressions, you can assume that the first $m$
values of $Y$ are observed and the remaining $n-m$ are missing. $2$) You
can use a built in function such as `optim` or the `maxLik` package for
the M-step.

### **Answer** :

```{r}
load("dataex4.Rdata")
head(dataex4)
cat("Number of missing values in Y:", sum(is.na(dataex4)))
```
Scrutinising on the dataset, we can observe that the missing value only occurs in $Y$ and there are $95$ missing values occurring in a univariate pattern

We first derive the likelihood function to implement the EM algorithm given that $y_{obs}=y_1,\dots,y_m$ and $y_{mis}=y_{m+1},\dots,y_{n}$.

\begin{equation}
  \begin{split}
    L(\beta_0, \beta_1;\boldsymbol{x,y_{obs},y_{mis}}) &= \prod^{n}_{i=1} \bigg([p_i(\beta_0,\beta_1)]^{y_i}[1-p(\beta_0,\beta_1)]^{1-y_i}\bigg)\\
    \implies L(\beta_0,\beta_1;\boldsymbol{x,y_{obs},y_{mis}}) &= \prod^{n}_{i=1}\bigg( \frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}} \bigg)^{y_i} \bigg(\frac{1}{1+e^{\beta_0+x_i\beta_1}}\bigg)^{1-y_i} \\
    \implies \log L(\beta_0,\beta_1;\boldsymbol{x,y_{obs},y_{mis}}) &= \sum^{n}_{i=1} \bigg(y_i\log\bigg(\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\bigg)+(1-y_i)\log\bigg(\frac{1}{1+e^{\beta_0+x_i\beta_1}}\bigg) \bigg)\\
    \implies \log L(\beta_0,\beta_1;\boldsymbol{x,y_{obs},y_{mis}}) &=
    \sum^{n}_{i=1}\bigg(y_i\log(e^{\beta_0+x_i\beta_1})-\log(1+e^{\beta_0+x_i\beta_1})-y_i\log(1+e^{\beta+x_i\beta_1})+y_i\log(1+e^{\beta+x_i\beta_1}) \bigg) \\
    \implies \log L(\beta_0,\beta_1;\boldsymbol{x,y_{obs},y_{mis}}) &= \sum^{n}_{i=1} \bigg(y_i(\beta_0+x_i\beta_1)-\log(1+e^{\beta_0+x_i\beta_1}) \bigg) \\
    &= l(\boldsymbol{\beta};\boldsymbol{x,y_{obs},y_{mis}})
  \end{split}
\end{equation}

Then the score function is give by,
\begin{equation}
  \begin{split}
    U(\beta_0)=\frac{d}{d\beta_0}l(\boldsymbol{\beta};\boldsymbol{x,y_{obs},y_{mis}}) = \sum^{n}_{i}\bigg(y_i-\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\bigg) \\
    U(\beta_1)=\frac{d}{d\beta_1}l(\boldsymbol{\beta};\boldsymbol{x,y_{obs},y_{mis}}) =
  \end{split}
\end{equation}

Now we proceed to implement the EM algorithm by calculating $Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}})$
\begin{equation}
  \begin{split}
    Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}})&=\mathbb{E}_{\boldsymbol{y_{mis}}}
    [l(\boldsymbol{\beta};\boldsymbol{x, y_{obs}, y_{mis}})|\boldsymbol{y_{obs}, x, \beta^{(t)}}] \\
    &=\sum^{m}_{i=1}\bigg(y_i(\beta_0 + x_i\beta_i)\bigg)-\sum^{m}_{i=1}\bigg(\log(1 + e^{\beta_0 + x_i\beta_1})\bigg) + \sum^{n}_{i=m+1}\bigg( (\beta_0+x_i\beta_1) \mathbb{E}_{\boldsymbol{y_{mis}}}[y_i|\boldsymbol{x, y_{obs}, \beta^{(t)}}] \bigg) \\
    &=\sum^{m}_{i=1}\bigg(y_i(\beta_0+x_i\beta_i)\bigg)-\sum^{m}_{i=1}\bigg(\log(1 + e^{\beta_0 + x_i\beta_1})\bigg) + \sum^{n}_{i=m+1}\bigg( (\beta_0 + x_i\beta_1)p_i(\beta) \bigg) \\
    &(\mathbb{E}(Y_i)=p_i(\boldsymbol{\beta}) \text{ as } Y_i\sim\text{Bernoulli}(p_i(\boldsymbol{\beta})))
  \end{split}
\end{equation}

Now we differentiate $Q$ with respect to $\beta_0$ and $\beta_1$
\begin{equation}
  \begin{split}
    \frac{d}{d\beta_0}Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}}) &=\sum^{m}_{i=1}\bigg(y_i-\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\bigg) + \sum^{n}_{i=m+1}\bigg(\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}+x_i\beta_1\frac{e^{\beta_0+x_i\beta_1}}{(1+e^{\beta_0+x_i\beta_1})^2}+\beta_0\frac{e^{\beta_0+x_i\beta_1}}{(1+e^{\beta_0+x_i\beta_1})^2} \bigg)\\
    \frac{d}{d\beta_1}Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}}) &=\sum^{m}_{i=1}\bigg(y_ix_i-x_i\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\bigg) + \sum^{n}_{i=m+1}\bigg(\beta_0\frac{x_ie^{\beta_0+x_i\beta_1}}{(1+e^{\beta_0+x_i\beta_1})^2}+x_i\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}+x_i\beta_1\frac{x_ie^{\beta_0+x_i\beta_1}}{(1+e^{\beta_0+x_i\beta_1})^2} \bigg)\\
  \end{split}
\end{equation}

The solutions of the derivatives have no closed form expression and thus we need to resort to numerical methods. 
Before we proceed to the code, we first need to preprocess `dataex4` by arranging it.
```{r}
dataex4 <- dataex4[order(dataex4$Y),]
row.names(dataex4) <- NULL
head(dataex4,5)
tail(dataex4,5)
```


In the code, we have used for the stopping criterion as below
<!-- $$|p^{(t+1)}-p^{(t)}|+-->$$|\beta_0^{(t+1)}-\beta_0^{(t)}|+|\beta_1^{(t+1)}-\beta_1^{(t)}|<\varepsilon$$ 

```{r warning=FALSE}
log.lik.bernoulli <- function(param, data){
  beta0 <- param[1]; beta1 <- param[2]
  x <- data[, 1]; y <- data[, 2]
  express <- beta0+x[1:405]*beta1
  express.na <- beta0+x[406:500]*beta1
  # sum(y[1:405]-(express/(1+express)))+sum((express/(1+express))+x[406:500]*(express/(1+express)^2)+beta0*(express/(1+express)^2))
  sum(y[1:405]*express-log(1+express)+express.na*express.na/(1+express.na))
}

mle <- maxLik(logLik = log.lik.bernoulli, data=dataex4, start=c(0, 0))$estimate
# mle <- optim(c(0, 0), log.lik.bernoulli, data = dataex4, control = list(fnscale = -1), hessian = TRUE)$par
beta <- c(0,0)
i <- TRUE
diff <- 1
while(diff > 0.00001){
#     mle <- optim(beta, log.lik.bernoulli, data = dataex4,
# control = list(fnscale = -1), hessian = TRUE)$par
    mle <- maxLik(logLik = log.lik.bernoulli, data=dataex4, start = beta)
    diff <- sum(abs(mle$estimate-beta))
    print(diff)
    beta <- mle$estimate
  # if(diff >0.00001){
  # 
  # }
  # else{
  #   i==FALSE
  # }
}
beta

```


\newpage

## Q5

Consider a random sample $Y_1,...,Y_n$ from the mixture distribution
with density
$$f(y) = pf_{\text{LogNormal}}(y;\mu, \sigma^2) + (1-p)f_{\text{Exp}}(y;\lambda),$$

with \begin{equation*}
  \begin{split}
  f_{\text{LogNormal}}(y;\mu, \sigma^2) &= \frac{1}{y\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y - \mu)^2\right\}, 
  \quad y>0, \quad \mu \in \mathbb{R}, \; \sigma > 0 \\
  f_{\text{Exp}}(y;\lambda) &= \lambda e^{-\lambda y}, \quad y \geq 0, \quad \lambda > 0
  \end{split}
\end{equation*}

and $\boldsymbol{\theta} = (p, \mu, \sigma^2, \lambda)$ 

## a) 
Derive the EM algorithm to find the updating equations for $\boldsymbol{\theta^{(t+1)}} = (p^{(t+1)}, \mu^{(t+1)}, (\sigma^{(t+1)})^2, \lambda^{(t+1)})$.

### **Answer** :

Let us consider a mixture model of Log-Normal and Exponential distributions.
$$\mathbb{P}(Y\leq y)=p\cdot\frac{1}{y\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y - \mu)^2\right\}+(1-p)\cdot\lambda e^{-\lambda y}$$

Let $z_i$ be the binary latent variables indicating component membership, i.e.

\begin{equation*}
  z_i=
  \begin{cases}
    1 & \text{if } y_i \text{ belong to } f_{\text{LogNormal}}(y;\mu, \sigma^2) \\
    0 & \text{if } y_i \text{ belong to } f_{\text{Exp}}(y;\lambda)
  \end{cases}
\end{equation*}

The observed data in this context is $\boldsymbol{y}=(y_1\dots y_n)$ and the missing data is $\boldsymbol{z}=(z_1\dots z_n)$. The likelihood of the complete data $(\boldsymbol{y,z})$ is
\begin{equation}
  \begin{split}
    L(\theta;\boldsymbol{y,z})&=\prod^{n}_{i=1}\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y_i - \mu)^2\right\}\bigg)^{z_i}\bigg((1-p)\cdot\lambda e^{-\lambda y_i}\bigg)^{1-z_i} \\
    \implies \log L(\theta;\boldsymbol{y,z})&=\sum^{n}_{i=1}z_i\log\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y_i - \mu)^2\right\}\bigg)+\sum^{n}_{i=1}(1-z_i)\log\bigg((1-p)\cdot\lambda e^{-\lambda y_i}\bigg)
  \end{split}
\end{equation}

with the corresponding log likelihood, we proceed to E-Step,

\begin{equation}\label{eq:qfn}
  \begin{split}
    Q(\boldsymbol{\theta};\boldsymbol{\theta^{(t)}})&=\mathbb{E}_{Z}(\log L(\theta;\boldsymbol{y,z})|\boldsymbol{y,\theta^{(t)}})\\
    &=\sum^{n}_{i=1}\mathbb{E}(Z_i|y_i,\theta^{(t)})\log\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y_i - \mu)^2\right\}\bigg)\\
    &+\sum^{n}_{i=1}(1-\mathbb{E}(Z_i|y_i,\theta^{(t)}))\log\bigg((1-p)\cdot\lambda e^{-\lambda y_i}\bigg)
  \end{split}
\end{equation}

We know that $\mathbb{E}(Z_i|\boldsymbol{y,\theta^{(t)}})=\mathbb{P}(Z_i=1|y_i,\theta^{(t)})$, and applying Bayes Theorem and the Law of Total Probability, we obtain,

\begin{equation}\label{eq:sub}
  \begin{split}
    \mathbb{E}(Z_i|\boldsymbol{y,\theta^{(t)}})&=\mathbb{P}(Z_i=1|y_i,\theta^{(t)})\\
    &=\frac{\bigg(p^{(t)}\cdot\frac{1}{y_i\sqrt{2\pi(\sigma^2)^{(t)}}}\text{exp}\left\{\frac{1}{2(\sigma^2)^{(t)}}(\log y_i - \mu^{(t)})^2\right\}\bigg)}{\bigg(p\cdot\frac{1}{y_i\sqrt{2\pi(\sigma^2)^{(t)}}}\text{exp}\left\{\frac{1}{2(\sigma^2)^{(t)}}(\log y_i - \mu^{(t)})^2\right\}\bigg)\bigg((1-p^{(t)})\cdot\lambda^{(t)} e^{-\lambda^{(t)} y_i}\bigg)}\\
    &= \tilde{p}_i^{(t)}
  \end{split}
\end{equation}
Therefore, we substitute \ref{eq:sub} into \ref{eq:qfn}
\begin{equation}
  \begin{split}
    
  \end{split}
\end{equation}

\newpage

## b)

Using the dataset `datasetex5.Rdata` implement the EM algorithm and find
the MLEs for each component of $\theta$. As starting values, you might
want to consider $\theta^{(0)} = (p^{(0)}, \mu^{0)}, (\sigma^{(0)})^2, \lambda^{(0)}) = (0.1, 1, 0.5^2, 2)$. Draw the histogram of the data with the estimated density superimposed.

### **Answer** :

```{r}
load("dataex5.Rdata")
```

