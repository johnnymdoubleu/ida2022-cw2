---
title: "IDA Assignment 2"
author: "Johnny Lee, s1687781"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", "English")
library(maxLik)
load("dataex2.Rdata")
load("dataex4.Rdata")
load("dataex5.Rdata")
```

# Q1.

Suppose $Y_1, \cdots , Y_n$ are independent and identically distributed
with cumulative distribution function given by
$$F(y;\theta) = 1 - e^{-y^2/(2\theta)}, \quad y \geq 0, \quad \theta > 0.$$
Further suppose that observations are (right) censored if $Y_i > C$, for
some known $C > 0$, and let \begin{equation*}
  X_i=
  \begin{cases}
    Y_i \quad \text{if} \quad Y_i \leq C,\\
    C \quad \text{if} \quad Y_i > C,
  \end{cases}
  \quad
  R_i=
  \begin{cases}
    1 \quad \text{if} \quad Y_i \leq C\\
    0 \quad \text{if} \quad Y_i > C
  \end{cases}
\end{equation*}

## a)

Show that the maximum likelihood estimator based on the observed data
$\{(x_i,r_i)\}^{n}_{i=1}$ is given by
$$\hat{\theta} = \frac{\sum_{i=1}^n X_i^2}{2\sum_{i=1}^nR_i}.$$

### **Answer** :

We first define the Survival function (from **Workshop 3**)as
$$S(C;\theta)=\mathbb{P}(Y_i>C;\theta)=1-F(y_i;\theta)$$ which also
represents the censored observations. For the uncensored observations,
we have
$$f(y_i;\theta)=\frac{d}{dy_i}F(y_i;\theta)=\frac{ye^{-y^2/2\theta}}{\theta}$$
Given that $Y_1,\dots,Y_n$ are independent and identically distributed,
we have the likelihood function as,
\begin{equation}\label{eq:likelihood}
  \begin{split}
    L(\theta|\boldsymbol{y,r})&= \prod^{n}_{i=1} \bigg( [f(y_i;\theta)]^{r_i} [S(C;\theta)]^{1-r_i}\bigg)\\
    &= \prod^{n}_{i=1} \bigg( [\frac{ye^{-y^2/2\theta}}{\theta}]^{r_i} [e^{-C^2/\theta}]^{1-r_i}\bigg)\\
    &= \bigg( \frac{y_i}{2\theta}\bigg)^{\sum^{n}_{i=1}r_i}\exp\bigg(\frac{\sum^{n}_{i=1}(r_i y^2_i +(1-r_i)C^2)}{2\theta}\bigg)
  \end{split}
\end{equation}

Now we can rewrite the term in the exponential as $X_i$ can we expressed
as $$x_i=r_iy_i+C(1-r_i)$$ Then by taking square on both sides we have,
$$x_i^2=r_i^2y_i^2+(1-r_i)^2C^2+2r_iy_iC(1-r_i)$$ Noting that $R_i$ is
binary, we can then conclude with the expression as
\begin{equation} \label{eq:x}
  \begin{split}
    x_i^2=r_iy_i^2+(1-r_i)C^2
  \end{split}
\end{equation}

Now we substitute (\ref{eq:x}) into (\ref{eq:likelihood}) to have,
\begin{equation}\label{eq:loglik}
  \begin{split}
    L(\theta|\boldsymbol{y,r}) &= \bigg( \frac{y_i}{2\theta}\bigg)^{\sum^{n}_{i=1}r_i}\exp\bigg(\frac{\sum^{n}_{i=1}x_i}{2\theta}\bigg)\\
    \implies \log(L(\theta|\boldsymbol{y,r}))&=-\log\theta\sum^{n}_{i=1}r_i-\frac{\sum^{n}_{i=1}x_i^2}{2\theta}\\
    \implies \frac{d}{d\theta}\log L(\theta|\boldsymbol{y,r})&=\frac{1}{\theta}\sum^{n}_{i=1}r_i + \frac{1}{2\theta^2}\sum^{n}_{i=1}x_i^2
    \end{split}
\end{equation} By equating the derivative to $0$, we can obtain the
maximum likelihood estimate of $\theta$ as below.
$$\hat{\theta}_{MLE} = \frac{\sum_{i=1}^n x_i^2}{2\sum_{i=1}^nr_i} \quad \text{(shown)}$$.

\newpage

## b)

Show that the expected Fisher Information for the observed data
likelihood is $$I(\theta) = \frac{n}{\theta^2}(1 - e^{-C^2/(2\theta)})$$

**Note:**
$\int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/(2\theta)} + 2\theta(1 - e^{-C^2 / (2\theta)})$,
where $f(y;\theta)$ is the density function corresponding to the
cumulative distribution function $F(y;\theta)$ defined above.

### **Answer** :

From (\ref{eq:loglik}), we take another derivative of it and thus obtain
as below
$$\frac{d^2}{d\theta^2}\log L(\theta)=\frac{1}{\theta^2}\sum^{n}_{i=1}r_i-\frac{x_i^2}{\theta^3}$$
Then, the Fisher Information for the observed data likelihood is,
\begin{equation}\label{eq:fi}
  \begin{split}
    I(\theta)&=-\mathbb{E}\bigg(\frac{\sum^{n}_{i=1}r_i}{\theta^2}-\frac{x^2_i}{\theta^3}\bigg)\\
    &=-\frac{n\mathbb{E}(R)}{\theta^2}+\frac{n\mathbb{E}(X^2)}{\theta^3}\\
    &=-\frac{n\mathbb{E}(R)}{\theta^2}+\frac{1}{\theta^3}\bigg(n\mathbb{E}(RY^2)+nC^2\mathbb{E}(1-R)\bigg)
  \end{split}
\end{equation}

Again, noting that $R_i$ is binary, \begin{equation}\label{eq:expR}
  \begin{split}
    \mathbb{E}(R)&=1\cdot\mathbb{P}(R=1)+0\cdot\mathbb{P}(R=0)\\
    &=\mathbb{P}(R=1)=\mathbb{P}(Y\leq C)\\
    &=F(C;\theta) = 1-e^{-C^2/2\theta}
  \end{split}
\end{equation}

With the given equation,
$\mathbb{E}(RY^2)=\int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/(2\theta)} + 2\theta(1 - e^{-C^2 / (2\theta)})$,
we can combine all the above equations as express the expected Fisher
Information again,

\begin{equation}
  \begin{split}
    I(\theta)&=\frac{n\mathbb{E}(R)}{\theta^2}+\frac{1}{\theta^3}\bigg(n\mathbb{E}(RY^2)+nC^2\mathbb{E}(1-R)\bigg)\\
    &=\frac{-n}{\theta^2}(1-e^{-C^2/2\theta})-\frac{n}{\theta^3}(C^2e^{-C^2/2\theta})+\frac{n}{\theta^3}(2\theta(1-e^{-C^2/2\theta}))+\frac{n}{\theta^3}(C^2e^{-C^2/2\theta}) \\
    &=\frac{n}{\theta^2}(1-e^{-C^2/2\theta}) \quad \text{(shown)}
  \end{split}
\end{equation}

\newpage

## c)

Appealing to the asymptotic normality of the maximum likelihood
estimator, provide a $95\%$ confidence interval for $\theta$.

### **Answer** :

Asymptotic normality of the maximum likelihood estimator is given as, $$
\hat{\theta}\sim N_p(0, I(\theta)^{-1})
$$ Thus, with $0$ and $\frac{1}{I(\theta)}$ as mean and variance
respectively, we can obtain the $95\%$ confidence interval as below,
$$\hat{\theta}\pm \frac{1.96}{\sqrt{I(\theta)}}$$

\newpage

# Q2.

Suppose that a dataset consists of $100$ subjects and $10$ variables.
Each variable contains $10\%$ of missing values. What is the largest
possible subsample under a complete case analysis? What is the smallest?
Justify.

Suppose that $Y_i \overset{\text{iid}}{\sim} N(\mu, \sigma^2)$ are iid
for $i=1,...,n$. Further suppose that now observations are (left)
censored if $Y_i < D$, for some known $D$ and let

\begin{equation*}
  X_i=
  \begin{cases}
    Y_i \quad \text{if} \; Y_i \geq D,\\
    D \quad \text{if} \; Y_i < D, 
  \end{cases}
  \quad
  R_i=
  \begin{cases}
    1 \quad \text{if} \; Y_i \geq D\\
    0 \quad \text{if} \; Y_i < D
  \end{cases}
\end{equation*}

## a)

Show that the log-likelihood of the observed data
$\{(x_i,r_i)\}^{n}_{i=1}$ is given by

$$\log L(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r}) = \sum_{i=1}^n\left\{r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu,     \sigma^2)\right\}$$

where $\phi(\cdot;\mu,\sigma^2)$ and $\Phi(\cdot;\mu, \sigma^2)$ stands,
respectively, for the density function and cumulative distribution
function of the normal distribution with mean $\mu$ and variance
$\sigma^2$.

### **Answer** :

We first define the Survival function (from **Workshop 3**)as 
$$S(D;\mu,\sigma^2)=\mathbb{P}(Y_i<D;\mu,\sigma^2)=\Phi(x_i;\mu,\sigma^2)$$
which also represents the censored observations. For the uncensored observation, we have
$$\phi(x_i;\mu,\sigma^2)$$

Given that $X_1,\dots,X_n$ are independent and identically distributed,
we have the likelihood function as,


\begin{equation}
  \begin{split}
    L(\mu,\sigma^2|\boldsymbol{x,r}) &= \prod^{n}_{i=1}\bigg([\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu,\sigma^2)]^{1-r_i}\bigg)\\
    \implies l(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r})&=\log \prod_{i=1}^{n}\bigg(\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu, \sigma^2)]^{1-r_i}\bigg)\\
    &= \log \bigg([\phi(x_i;\mu,\sigma^2)]^{\sum^{n}_{i=1} r_i}[\Phi(x_i;\mu, \sigma^2)]^{\sum^{n}_{i=1}(1-r_i)}\bigg)\\
    &=  \sum_{i=1}^n\bigg(r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu, \sigma^2)\bigg)
  \end{split}
\end{equation}

\newpage

## b)

Determine the maximum likelihood estimate of $\mu$ based on the data
available in the file `dataex2.Rdata`. Consider $\sigma^2$ known and
equal to $1.5^2$. **Note**: You can use a built in function such as
`optim` or the `maxLik` package in your implementation.

### **Answer** :

```{r}
#defining a function to simulate the log likelikhood
log.lik <- function(mu, data){
  x <- data[, 1]
  r <- data[, 2]
  sum((r*dnorm(x, mu, 1.5, log = TRUE) + 
         (1-r)*pnorm(x, mu, 1.5, log = TRUE)))
}

#computing the maximum likelihood estimate of mu
mle <- maxLik(logLik = log.lik, data = dataex2, start = c(mu = 5))
summary(mle)
```

We built a function `log.lik()` that produces the log likelihood and then used `maxLik()` to simulate $\mu$ based on the data. With Newton-Raphson method, we estimated $\hat{\mu}=5.5328$ and standard error of $0.1075$

\newpage

# Q3.

Consider a bivariate normal sample $(Y_1, Y_2)$ with parameters
$\theta=(\mu_1,\mu_2,\sigma_1^2,\sigma_{12},\sigma_2^2)$. The variable
$Y_1$ is fully observed, while some values of $Y_2$ are missing. Let $R$
be the missingness indicator, taking the value 1 for observed values and
0 for missing values. For the following missing data mechanisms state,
justifying, whether they are ignorable for likelihood-based estimation.

## a)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=\psi_0+\psi_1y_1, \quad \psi= (\psi_0,\psi_1)$
distinct from $\theta$.

### **Answer** :

Referring to the ignorability assumption (from **Lecture 6.1**), the missing in $Y_2$ is either **MAR** or **MCAR** and its model parameters, $\theta = (\mu_1, \mu_2, \sigma_1^2, \sigma_{12}, \sigma_2^2)$ and missing mechanism parameter, $\psi$.

First, the missing mechanism is **MAR**. This is because the missingness is only dependent on $Y_1$ which is a fully observed variable. The parameters, $\{\theta,\psi\}$ are also distinct. Therefore, the ignorability assumption holds here and (a) is ignorable for likelihood-based estimation.

## b)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=\psi_0+\psi_1y_2, \quad \psi= (\psi_0,\psi_1)$
distinct from $\theta$.

### **Answer** :

The missing mechanism is **MNAR** as the mechanism is only dependent on $Y_2$. Therefore, the missing value is depending on itself and possibly other factors. Therefore, by referring to the ignorability assumption (from **Lecture 6.1**), we conclude that (b) is not ignorable for likelihood-based estimation.

### c)

$\text{logit}\{\mathbb{P}(R=0|y_1,y_2,\theta,\psi)\}=0.5(\mu_1+\psi y_1)$,
scalar $\psi$ distinct from $\theta$.

### **Answer** :

The missing mechanism here is dependent on both $\mu_1$ and $Y_1$. We can observe similarity to (a). Distinctness of the parameters means that the parameter space of $(\theta, \psi)$ is equal to the Cartesian product of their individual product spaces. However, the $\mu_1$ also exists in the parameter space. This violates the ignorability assumption. Hence, (c) is not ignorable for likelihood-based estimation

\newpage

# Q4.

$$Y_i\overset{\text{ind.}}{\sim} \text{Bernoulli}(p_i(\boldsymbol{\beta})$$

$$p_i(\boldsymbol{\beta}) = \frac{exp(\beta_0 + x_i\beta_1)}{1 + exp(\beta_0 + x_i\beta_1)},$$
for $i=1,\cdots,n$ and $\boldsymbol{\beta} = (\beta_0, \beta_1)'$.
Although the covariate $x$ is fully observed, the response variable $Y$
has missing values. Assuming ignorability, derive and implement the EM
algorithm to compute the MLE of $\boldsymbol{\beta}$ based on the data
available in `dataex4.Rdata`. **Note**: $1$) For simplicity, and without
loss of generality because we have a univariate pattern of missingness,
when writing down your expressions, you can assume that the first $m$
values of $Y$ are observed and the remaining $n-m$ are missing. $2$) You
can use a built in function such as `optim` or the `maxLik` package for
the M-step

### **Answer** :



\newpage

## Q5

Consider a random sample $Y_1,...,Y_n$ from the mixture distribution
with density
$$f(y) = pf_{\text{LogNormal}}(y;\mu, \sigma^2) + (1-p)f_{\text{Exp}}(y;\lambda),$$

with \begin{equation*}
  \begin{split}
  f_{\text{LogNormal}}(y;\mu, \sigma^2) &= \frac{1}{y\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y - \mu)^2\right\}, 
  \quad y>0, \quad \mu \in \mathbb{R}, \; \sigma > 0 \\
  f_{\text{Exp}}(y;\lambda) &= \lambda e^{-\lambda y}, \quad y \geq 0, \quad \lambda > 0
  \end{split}
\end{equation*}

and $\boldsymbol{\theta} = (p, \mu, \sigma^2, \lambda)$ ## a) Derive the
EM algorithm to find the updating equations for
$\boldsymbol{\theta^{(t+1)}} = (p^{(t+1)}, \mu^{(t+1)}, (\sigma^{(t+1)})^2, \lambda^{(t+1)})$.

### **Answer** :

## b)

Using the dataset `datasetex5.Rdata` implement the EM algorithm and find
the MLEs for each component of $\theta$. As starting values, you might
want to consider
$\theta^{(0)} = (p^{(0)}, \mu^{0)}, (\sigma^{(0)})^2, \lambda^{(0)}) = (0.1, 1, 0.5^2, 2)$.
Draw the histogram of the data with the estimated density superimposed.
##\# **Answer** :
